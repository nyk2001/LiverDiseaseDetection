---
title: "Liver Disease Classification"
author: "Nabeel Khan"
date: "27-May-2020"
output:
  pdf_document: 
    keep_tex: true
    number_sections: yes
    toc: yes
    highlight: tango
    df_print: kable
  html_document:
    df_print: paged
    number_sections: yes
    toc: yes
---
\section{Introduction}
\label{sec:introduction}
This project is part of the ‘HarvardX: PH125.9x Data Science: Capstone’ course. In this project, we chose a dataset of our choice and develop machine learning models to perform binary classification to diagnose liver disease. 

\subsection{Background}
\label{sec:background}
The liver plays a vital role in keeping us healthy. The liver's main job is to filter the blood coming from the digestive tract before passing it to the rest of the body. The liver also turns nutrients into chemicals our body needs, converts food into energy, and filters out poisons. The damage to the liver affects the whole body.

The patients with liver disease are on the rise because of excessive consumption of alcohol, inhale of harmful gases, or intake of contaminated food. The problems with the liver are not easily discovered in an early stage. Moreover, the diagnosis of liver damage is subjective and varies from doctor to doctor based on experience.  The initial diagnosis of liver disease increases the survival rate of patients. The liver disease can be detected by analysing the levels of enzymes in the human blood \cite{ld,bendi}. Therefore, a classification algorithm capable of automatically detecting the liver disease can assist the doctors in diagnosing liver disease. The classification techniques are commonly used in various automatic medical diagnoses tools\cite{cad}.  

\subsection{Aim of Project}
\label{sec:aim}
This project aims to develop a binary classifier, which can use blood enzymes information to diagnose liver disease. That information will assist doctors in conducting further testing of patients for additional verification and start treatment in time to save lives.

\section{Dataset and Evaluation Metrics}
\label{sec:dataset}
We use the liver patient records, which are collected from North East of Andhra Pradesh, India. The data set contains:
\begin{enumerate}
\item 416 liver patient records and 167 non-liver patient records.
\end{enumerate} 

\subsection{Download Data}
\label{sec:dd}
The dataset is publically available online both at Kaggle and UCI repository.
We download data from the website. Then, we split data into training and validation sets. 
\begin{itemize}
\item 10\% of the data is used for validation, and 90% is used for training.
\end{itemize}

```{r,message=FALSE, warning=FALSE}
################################
#  Install packages (if not installed)
################################
repos_path<- "http://cran.us.r-project.org"
if(!require(tidyverse)) install.packages("tidyverse", repos =repos_path)
if(!require(caret)) install.packages("caret", repos = repos_path)
if(!require(data.table)) install.packages("data.table", repos =repos_path)
if(!require(lubridate)) install.packages("lubridate", repos = repos_path)
if(!require(dplyr)) install.packages("dplyr", repos = repos_path)
if(!require(sjmisc)) install.packages("dplyr", repos = repos_path)
if(!require(scales)) install.packages("scales", repos = repos_path)
if(!require(caret)) install.packages("caret", repos = repos_path)
if(!require(gam)) install.packages("gam", repos = repos_path)

################################
# Load libraries
################################
library(lubridate)
library(tidyverse)
library(dplyr)
library(lubridate)
library(sjmisc)
library(scales)
library(caret)
library(gam)
```

```{r,message=FALSE, warning=FALSE}
################################
# Downloading data
################################
# Indian Live Patient Records :
 # https://www.kaggle.com/uciml/indian-liver-patient-records/
 # https://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian Liver Patient Dataset (ILPD).csv

url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian Liver Patient Dataset (ILPD).csv"

# Download csv
liverData <- read.csv(url)

# Rename columns of csv (follow the webpage for naming convention)
colnames(liverData)<- c("Age","Gender","Total_Bilirubin","Direct_Bilirubin", "Alkaline_Phosphotase","Alamine_Aminotransferase","Aspartate_Aminotransferase",	"Total_Protiens","Albumin","Albumin_and_Globulin_Ratio","Dataset")

################################
# Creating training and validation sets
################################

# Validation set will be 10% of whole data
set.seed(1)
test_index <- createDataPartition(y = liverData$Dataset, times = 1, p = 0.1, list = FALSE)

training <- liverData[-test_index,]
validation <- liverData[test_index,]

 # Removing the objects from environment as no longer required
rm(liverData)

```

Both training and validation datasets have a distribution of patients with and with no disease.

```{r}
# Looking at distributions of liver disease
# 1 indicates that the liver is damage. While 2 means that the liver is healthy 
print("Training Data")
table(training$Dataset)
print("Validation Data")
table(validation$Dataset)

```

\subsection{Metrics}
\label{sec:metrics}
To evaluate the performance of classifiers, we will use the following metrics:

\begin{enumerate}
\item \textbf{Accuracy}
It is the ratio of the number of correct predictions to the total number of input samples.
\begin{equation}
Accuracy = \frac{True Positives + True Negatives} {Total Predictions}
\end{equation}

\item \textbf{Precision}
It is defined as the proportion of the true positives against all the positive
results.

\begin{equation}
Precision = \frac{Number of True Positives} {Number of True Positives + Number of False Positives}
\end{equation}

\item \textbf{Sensitivity}
It is also referred as true positive rate or recall. It is the proportion of true positives that are correctly identified.

\begin{equation}
Sensitivity = \frac{Number of True Positives} {Number of True Positives + Number of False Negatives}
\end{equation}


\item \textbf{Specificity}
It is the true negative rate. It is the proportion of true negatives that are
correctly identified.

\begin{equation}
Specificity = \frac{Number of True Negatives} {Number of True Negatives + Number of False Positives}
\end{equation}

\item \textbf{F1 Score}
F1 score is a function of Precision and Recall. F1 Score is a better measure to use if we need to seek a balance between Precision and Recall. And, there is an uneven class distribution.


\begin{equation}
F1 Score = 2 * \frac{Precision * Recall} {Precision + Recall}
\end{equation}

\item \textbf{Cohen's Kappa}
Cohen's Kappa (or simple Kappa) measures inter-rater reliability (sometimes called interobserver agreement). It is a measure of agreement between the two individuals. The kappa statistic measures the percentage of data values in the main diagonal of the table and then adjusts the values for the amount of agreement that could be expected due to chance alone.

\end{enumerate}

The values of all metrics range from 0 to 1. Higher the value better is the metric.

\section{Data Exploration}
\label{sec:exploration}
The dataset contains 11 variables, namely, "Age", "Gender'', "Total_Bilirubin", or "Alkaline_Phosphotase". The 'Dataset' variable indicates if the liver has a disease or not. For instance, a value of 1 means that the liver is damaged, while a value of 2 means that the liver is healthy. 


All other variables except "Age", "Gender", and "Dataset" represent the amount of enzymes or proteins in the blood. These variables (or a subset) will be used to train our machine learning models to make diagnoses. 

```{r}
head(training)
```

The training dataset has 523 records. We can see that the "Albumin_and_Globulin_Ratio" variable has 4 null values. The remaining variables do not contain any null values.

\begin{itemize}
\item The validation data also has no null values (confirmed via summary).
\end{itemize}

```{r}
sprintf("Rows of training dataset = %d", nrow(training))
print("=========================")
summary(training)
```

```{r}
print("Validation Dataset")
summary(validation)
```


\subsection{Data Wrangling}
\label{sec:dw}
\subsubsection{Remove null values}
The variable "Albumin_and_Globulin_Ratio" has four null values. We use the traditional data science approach and replace null values with the mean of the variable.

```{r}
# Replace null values with the mean of the variable
training$Albumin_and_Globulin_Ratio[is.na(training$Albumin_and_Globulin_Ratio)] <- mean(training$Albumin_and_Globulin_Ratio, na.rm=TRUE)
```

\subsubsection{Create LiverDisease Variable}
To improve readability, we create a new variable, namely, "LiverDisease", which will have one of the following values: 
\begin{enumerate}
\item Malignant (M) indicating that the patient has liver disease.
\item Benign (B) indicating that the patient has no liver disease.
\end{enumerate}


We further delete the "Dataset" variable as it is no longer needed. We apply these operations to both training and validation datasets.

```{r}
# Adding a new column, which will contain the disease information
# M -> Malignant
# B -> Benign
training <- transform(training, LiverDisease= ifelse(Dataset==1, "M","B"))
validation <- transform(validation, LiverDisease= ifelse(Dataset==1, "M","B"))

# Deleting the column 'Dataset' as no longer required
training<-within(training, rm(Dataset))
validation<-within(validation, rm(Dataset))

# Displaying the first six rows
head(training)
```
The training data has 28% of the patient records, which has no liver damage or dieases. The rest of the patients have a liver damage.

```{r}
summary(training$LiverDisease)/nrow(training)*100.0
```

\section{Data Analysis}
\label{sec:dataanalysis}
In this section, we extract insights from all variables to get an in-depth understanding before using them to train the machine learning models.

\subsection{Age}
The dataset consists of patients with varying ages ranging from 4 to 90. The distribution of ages shows a nice spread and indicates that the dataset is unbiased towards a specific age group. 
```{r}
sprintf("Minimum age = %d",min(training$Age))
sprintf("Maximum age = %d",max(training$Age))
```


```{r}
# Extracting frequency of patient ages
age_stats <-as.data.frame(table(training$Age))
names(age_stats)<- c("Age","Count")

# Remvoing the factor
age_stats$Age<-as.numeric(levels(age_stats$Age))

# Plotting distribution of ages
age_stats %>% ggplot(aes(Age, Count)) +
  geom_point(color="cadetblue") +
  scale_x_continuous(breaks = round(seq(min(age_stats$Age), 
                                        max(age_stats$Age), by = 6),1)) +
  ggtitle("Distribution of Patient Ages")

```
Now, we breakdown the distribution of ages to the presence or absence of liver diseases. We notice a good spread of age group for both scenarios.

```{r}
# Plotting distributions of ages based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Age, color=LiverDisease)) +
  geom_point() +
  labs(y="Age", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of ages based on liver disease")
```

\subsection{Gender}
76% of the patient records are of males. It would be good to have a more and less equal distribution of records for both genders, although we do not expect it to make any difference in the performance of our models.  

Both "Gender" and "Age" variables are not used to train the model. These variables provide descriptive information.

```{r}
# Getting summary of genders
summary(training$Gender)
```

\subsection{Total_Bilirubin and Direct_Bilirubin}
Bilirubin refers to any form of a yellowish pigment made in the liver when red blood cells are broken down. The elevated levels of bilirubin indicate that the liver is damaged. We find a similar trend with these variables that levels of bilirubin are high for patients with liver diseases.  

```{r}
# Plotting distributions of Total_Bilirubin based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Total_Bilirubin, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Total_Bilirubin", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Total_Bilirubin based on liver disease")

```

```{r}
# Plotting distributions of Direct_Bilirubin based on liver disease
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Direct_Bilirubin, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Direct_Bilirubin", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Direct_Bilirubin based on liver disease")

```

Now, we look at the correlations of bilirubins. We observe that both bilirubins are weakly correlated with liver disease. However, both bilirubins are highly correlated, and we can also use one of them to train the model (if required).

```{r}
# Making a subset of data
subset_train <- training[c("Total_Bilirubin","Direct_Bilirubin","LiverDisease")]
# Converting disease variable to numeric format
subset_train <- transform(subset_train, LiverDisease= ifelse(subset_train$LiverDisease=="M", 1,0))
# Looking at the correlations
cor(subset_train) 
```
\subsection{Alkaline Phosphotase}
Alkaline phosphatase (ALP) is an enzyme in a person's blood that helps break down proteins. The elevated levels indicate that the liver has a disease, and we notice a similar trend in our dataset.  

```{r}
# Plotting distributions of Alkaline Phosphotase based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Alkaline_Phosphotase, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Alkaline Phosphotase", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Alkaline Phosphotase based on liver disease")
```
 
\subsection{Alamine_Aminotransferase and Aspartate_Aminotransferase}
Aminotransferases are enzymes that are important in the synthesis of amino acids, which form proteins. Alanine aminotransferase (ALT) and Aspartate aminotransferase (AST) are found primarily in the liver and kidney. High levels of ALT and AST are expected for patients with liver diseases. We also notice slightly elevated levels of these enzymes for patients with liver diseases in our dataset. 

```{r}
# Plotting distributions of Alamine Aminotransferase based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Alamine_Aminotransferase, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Alamine Aminotransferase", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Alamine Aminotransferase based on liver disease") 
```

```{r}
# Plotting distributions of Aspartate_Aminotransferase based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Aspartate_Aminotransferase, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Aspartate Aminotransferase", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Aspartate Aminotransferase based on liver disease") 
```

Contrary to bilirubins, there exists a weak correlation between both aminotransferases. 

```{r}
# Making a subset of data
subset_train <- training[c("Alkaline_Phosphotase","Aspartate_Aminotransferase","LiverDisease")]

# Converting disease variable to numeric format
subset_train <- transform(subset_train, LiverDisease= ifelse(subset_train$LiverDisease=="M", 1,0))

# Looking at the coorelations
cor(subset_train) 
```

\subsection{Total Protiens}
The total protein test measures the total amount of protein in your body. The distributions indicate that this variable cannot be used to diagnose liver disease. We do not see any pattern which we can use for classification.  


```{r}
# Plotting distributions of Total Protiens based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Total_Protiens, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Total Protiens", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Total Protiens based on liver diseases") 
```

\subsection{Albumin}
Albumin is a protein made by the liver to keep fluid in the bloodstream. The low levels of albumin indicate a problem with the liver, and we notice a similar trend in our dataset.

```{r}
# Plotting distributions of Albumin based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Albumin, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Albumin", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Albumin based on liver disease") 
```

\subsection{Albumin_and_Globulin_Ratio (AG)}
These proteins are crucial for body growth, development, and health. They form the structural part of most organs and makeup enzymes that regulate body functions. The low ratios of AG refer to liver issues, and we notice the same trend from distributions plot. 

```{r}
# Plotting distributions of Albumin based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Albumin_and_Globulin_Ratio, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Albumin and Globulin Ratio", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Albumin and Globulin Ratio based on liver disease") 
```

\section{Methods}
\label{sec:methods}
Based on the discussion in Section \ref{sec:dataanalysis}, we will not use "Age" and "Total Protein" variables to train the machine learning models. So, we remove these variables from both training and validation datasets. Then, we predict the liver disease using all the remaining variables of the dataset.

```{r}
# Removing the variables 'Age' and 'Dataset' 
training<-within(training, rm(Age,Total_Protiens))
validation<-within(validation, rm(Age,Total_Protiens))

```


For data pre-processing, we remove zero-variance predictors and then center and scale all those remaining using the preProc argument. Feature scaling is one of the most critical steps during the pre-processing of data before creating a machine learning model. It can make a significant difference between a weak machine learning model and a better one.

\subsection{Logistic Regression}
We use \emph{glm} method with cross-validation of 10 folds to train the model. 

```{r,warning=FALSE, message=FALSE}
# Defining a cross-validation (10 K folds )
control <- trainControl(method = "repeatedcv", number =10,repeats = 5)

# Train logistic regression model
train_glm <- train(LiverDisease ~., 
                   method = "glm",
                   preProc = c("zv","center", "scale"),
                   data = training,
                   trControl=control)

# Showing the accuracy
sprintf("The accuracy of GLM = %f",train_glm$results$Accuracy)

# Storing the results
model_results <- data_frame(method = "glm", Accuracy = train_glm$results$Accuracy)
```
\subsection{K-nearest neigbors (knn)}
We use \emph{knn} method with cross-validation of 10 folds to train the model. We tune the model with several values of \emph{k}, ranging from 3 to 51, to optimize the performance. 


```{r,warning=FALSE,message=FALSE}

set.seed(1)
# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number = 10, repeats=5)

# Train knn model
train_knn <- train(LiverDisease~ .,
                   method = "knn", 
                   preProc = c("zv","center", "scale"),
                   data = training,
                   tuneGrid = data.frame(k = seq(3, 51, 2)),
                   trControl = control)

# Plot the model and highlight the best result
ggplot(train_knn, highlight = TRUE) +
  ggtitle(paste("The best accuracy = ",round(max(train_knn$results$Accuracy),3)))

# Storing the results
model_results <- bind_rows(model_results,data_frame(method="knn",  
                                     Accuracy = max(train_knn$results$Accuracy) ))

```
\subsection{Local Regression}
We use \emph{loess} method with cross-validation of 10 folds to train the model. We tune the \textbf{span} and \textbf{degree} parameters to optimize the performance of the model.

```{r, warning=FALSE, message=FALSE}
set.seed(1)

# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number = 10, repeats=5)

# Define tuning parameters
tune_grid <- expand.grid(span = seq(0.15, 0.65, len = 15), degree = seq(0,1,0.25))

# Train the model
train_loess <- train(LiverDisease~.,
                   method = "gamLoess",
                   preProc = c("zv","center", "scale"),
                   data = training,
                   tuneGrid= tune_grid, 
                   trControl = control)

# Plot the model and highlight the best result
ggplot(train_loess, highlight = TRUE) +
  ggtitle(paste("The best accuracy = ",round(max(train_loess$results$Accuracy),3)))


# Storing the results
model_results <- bind_rows(model_results,data_frame(method="loess",  
                                     Accuracy = max(train_loess$results$Accuracy) ))


```

\subsection{Partial Least Squares (PLS)}
We use \emph{pls} method with cross-validation of 10 folds to train the model. We tune the \textbf{ncomp} parameter to optimize the performance of the model.

```{r, warning=FALSE, message=FALSE}
set.seed(1)

# Define tuning parameters
tune_grid <- expand.grid(ncomp = seq(1,5, len = 10))

# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number = 10, repeats=5)

# Train the model
train_pls <- train(LiverDisease~.,
                   method = "pls",
                   preProc = c("zv","center", "scale"),
                   data = training,
                   tuneGrid= tune_grid,
                   trControl = control)

# Plot the model and highlight the best result
ggplot(train_pls, highlight = TRUE) +
  ggtitle(paste("The best accuracy = ",round(max(train_pls$results$Accuracy),3)))


# Storing the results
model_results <- bind_rows(model_results,data_frame(method="pls",  
                                     Accuracy = max(train_pls$results$Accuracy) ))


```

\subsection{Linear Discriminant Analysis (LDA)}
The \emph{lda} is a statistical classifier, and we use this method with cross-validation of 10 folds for training. There is no parameter to tune for this model.

```{r, warning=FALSE, message=FALSE}
set.seed(1)
# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number=10, repeats=5)

# Train the model
train_lda <- train(LiverDisease~., 
                   method = "lda",
                   preProc = c("zv","center", "scale"),
                   data = training,
                   trControl = control)

sprintf("The accuracy of lda = %f",max(train_lda$results$Accuracy))

# Storing the results
model_results <- bind_rows(model_results,data_frame(method="lda",  
                                     Accuracy = max(train_lda$results$Accuracy) ))


```

\subsection{Quadratic Discriminant Analysis (QDA)}
The \emph{qda} is a statistical classifier, and we use the method with cross-validation of 10 folds for training. There is no parameter to tune for this model.

```{r, warning=FALSE, message=FALSE}
set.seed(1)
# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number=10, repeats=5)

# Train the model
train_qda <- train(LiverDisease~.,
                   method = "qda",
                   preProc = c("zv","center", "scale"),
                   data = training,
                   trControl = control)

sprintf("The accuracy of qda = %f",max(train_qda$results$Accuracy))

# Storing the results
model_results <- bind_rows(model_results,data_frame(method="qda",  
                                     Accuracy = max(train_qda$results$Accuracy) ))


```
\subsection{Decision Tress}

We use \emph{raprt} method with cross-validation of 10 folds for training. We tune the \emph{cp} parameter to optimize the performance of the model.

```{r, warning=FALSE, message=FALSE}
set.seed(1)

# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number = 10, repeats=5)

# Train the model
train_rpart <- train(LiverDisease~., 
                     method = "rpart",
                     preProc = c("zv","center", "scale"),
                     data = training,
                     trControl = control,
                     tuneGrid = data.frame(cp = seq(0, 0.08, len = 10)))

# Plot the model and highlight the best result
ggplot(train_rpart, highlight = TRUE) +
  ggtitle(paste("The best accuracy = ",round(max(train_rpart$results$Accuracy),3)))

# Storing the results
model_results <- bind_rows(model_results,data_frame(method="rpart",  
                                     Accuracy = max(train_rpart$results$Accuracy) ))


```
\subsection{Random Forests}
We use \emph{rf} method with cross-validation of 10 folds for training. We tune the \emph{mtry} parameter to optimize the performance of the model.

```{r, warning=FALSE, message=FALSE}
set.seed(1)

# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number = 10, repeats=5)

# Train the model
train_rf <- train(LiverDisease~.,
                     method = "rf",
                     preProc = c("zv","center", "scale"),
                     data = training,
                     trControl = control,
                     tuneGrid = data.frame(mtry=seq(1,7)),
                     ntree=100)

# Plot the model and highlight the best result
ggplot(train_rf, highlight = TRUE) +
  ggtitle(paste("The best accuracy = ",round(max(train_rf$results$Accuracy),3)))

# Storing the results
model_results <- bind_rows(model_results,data_frame(method="rf",  
                                     Accuracy = max(train_rf$results$Accuracy) ))

```
\subsection{Support Vector Machine}
We use support vector machine with cross-validation of 10 folds for training. We tune the \emph{tau} parameter to optimize the performance of the model.

```{r, warning=FALSE, message=FALSE}
set.seed(1)

# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

# Train the model
train_svm <- train(LiverDisease~.,
                   preProc = c("zv","center", "scale"),
                   data = training,
                   method = "svmLinear",
                   tune_grid= data.frame(tau=seq(1,10)),
                   trControl = control)

sprintf("The accuracy of svm = %f",max(train_svm$results$Accuracy))

# Storing the results
model_results <- bind_rows(model_results,data_frame(method="svm",  
                                     Accuracy = max(train_svm$results$Accuracy) ))

```

\subsection{Adaptive Boosting (Adaboost)}
AdaBoost is a machine learning meta-algorithm for classification. We train the model with cross-validation of 10 folds.

```{r}
set.seed(1)

# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

# Tuning the model
tune_grid  = expand.grid(method = c("Adaboost.M1", "Real adaboost"))
 
train_ada <- train(LiverDisease~.,
                   preProc = c("zv","center", "scale"),
                   data = training,
                   method = "ada",
                   trControl = control)

sprintf("The accuracy of adaboost = %f",max(train_ada$results$Accuracy))

# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="ada",  
                                     Accuracy = max(train_ada$results$Accuracy) ))

```

\subsection{Random Forest with PCA}
We apply principal component analysis on data and then use \emph{rf} method to train the model with cross-validation of 10 folds. We utilize the same tuning, which was used before with the random forest model.

```{r}
set.seed(1)

# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number = 10, repeats=5)
# Train the model
train_rf_pca <- train(LiverDisease~.,
                     method = "rf",
                     preProc = c("zv","center", "scale"),
                     data = training,
                     trControl = control,
                     tuneGrid = data.frame(mtry=seq(1,7)),
                     preProcess=c("pca"),
                     ntree=100)

# Plot the model and highlight the best result
ggplot(train_rf_pca, highlight = TRUE) +
  ggtitle(paste("The best accuracy = ",round(max(train_rf_pca$results$Accuracy),3)))

# Storing the results
model_results <- bind_rows(model_results,data_frame(method="rf_pca",  
                                     Accuracy = max(train_rf_pca$results$Accuracy) ))



```


The reported accuracies and kappas for all models across the training dataset are shown in the following table and graph. The results show that \emph{qda} model performs the worse. All other models provide an accuracy of around 0.70. The random forest, along with the principal component analysis, gives the best performance.

```{r}
model_results
```


```{r}
# collect resamples
results <- resamples(list(GLM=train_glm, 
                          KNN = train_knn,
                          LOESS=train_loess,
                          PLS= train_pls,
                          LDA = train_lda,
                          QDA = train_qda,
                          RPART = train_rpart,
                          RF = train_rf,
                          SVM = train_svm,
                          ADA= train_ada,
                          RF_PCA = train_rf_pca))
# boxplots of results
bwplot(results)
```


\section{Results}
\label{sec:results}
The statistical measurements of accuracy and precision reveal the necessary reliability of a test. Specificity is the ability of a test to exclude individuals who do not have a given disease correctly, and sensitivity is the ability of a test to identify people who have a given disease accurately. On the other hand, the F1 score is the harmonic mean of Precision and Recall and gives a better measure of the incorrectly classified cases than the accuracy metric. And, the kappa metric measures the inter-rater reliability.

In Section \ref{sec:methods}, we trained several models using training data. Now, we will evaluate the performance of these models using validation data. We have not used \textbf{qda} model due to poor performance on training data. The results show that \textbf{rf} and \emph{rf_pca} models performs best in terms of precision and recall. However, the models have slightly lower sensitivity and specificity compared to other models. But, the \textbf{rf_pca} model gives the best kappa value and is significantly better than the remaining ones. So, we can deduce that overall, \textbf{rf_pca} performs the best compared to other models.


```{r}
# Function to display a confusion matrix
# Code Source:
# https://stackoverflow.com/questions/23891140/r-how-to-visualize-confusion-matrix-using-the-caret-package

draw_confusion_matrix <- function(cm, title) {

  total <- sum(cm$table)
  res <- as.numeric(cm$table)

  # Generate color gradients. Palettes come from RColorBrewer.
  greenPalette <- c("#F7FCF5","#E5F5E0","#C7E9C0","#A1D99B","#74C476","#41AB5D","#238B45","#006D2C","#00441B")
  redPalette <- c("#FFF5F0","#FEE0D2","#FCBBA1","#FC9272","#FB6A4A","#EF3B2C","#CB181D","#A50F15","#67000D")
  getColor <- function (greenOrRed = "green", amount = 0) {
    if (amount == 0)
      return("#FFFFFF")
    palette <- greenPalette
    if (greenOrRed == "red")
      palette <- redPalette
    colorRampPalette(palette)(100)[10 + ceiling(90 * amount / total)]
  }

  # set the basic layout
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title(title, cex.main=2)

  # create the matrix 
  classes = colnames(cm$table)
  rect(150, 430, 240, 370, col=getColor("green", res[1]))
  text(195, 435, classes[1], cex=1.2)
  rect(250, 430, 340, 370, col=getColor("red", res[3]))
  text(295, 435, classes[2], cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col=getColor("red", res[2]))
  rect(250, 305, 340, 365, col=getColor("green", res[4]))
  text(140, 400, classes[1], cex=1.2, srt=90)
  text(140, 335, classes[2], cex=1.2, srt=90)

  # add in the cm results
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
 
  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}

```

```{r}
# Creating an empty data frame to hold the results of models 
# across validation dataset
ml_results<-data_frame()

# Function to compute all stats from models
evaluate_performance <- function(model_name, model, validation,model_results,title) 
{
  # Generating predictions
  predictions<-predict(model, validation)
  
   # Draw the confusion matrix
  cm<-confusionMatrix(predictions,validation$LiverDisease,positive="M")
  
  draw_confusion_matrix(cm,title)
 
  # Generate metrics
  sensitivty<-as.numeric(cm$byClass[1])
  specificity<-as.numeric(cm$byClass[2])
  precision<-as.numeric(cm$byClass[5])
  recall<-as.numeric(cm$byClass[6])
  f1_score<-as.numeric(cm$byClass[7])
  accuracy<-as.numeric(cm$overall[1])
  kappa <-as.numeric(cm$overall[2])
  
  # Store metrics to a data frame
  ml_results <- bind_rows(ml_results, data_frame(Models = model_name,
             Accuracy = accuracy,
             Precision= precision,
             Sensitivty=sensitivty,
             Specificity=specificity,
             F1_Score = f1_score,
             Kappa= kappa))
}

# Evaluating the performance of models
ml_results<-evaluate_performance("glm",train_glm,validation,ml_results,"Confusion Matrix - glm")

```
```{r}
# Evaluate knn model
ml_results<-evaluate_performance("knn",train_knn,validation,ml_results,"Confusion Matrix - knn")

```


```{r}
#Evaluate loess model
ml_results<-evaluate_performance("loess",train_loess,validation,ml_results,"Confusion Matrix - loess")
```

```{r}
ml_results<-evaluate_performance("pls",train_pls,validation,ml_results,"Confusion Matrix -pls")
```

```{r}
ml_results<-evaluate_performance("lda",train_lda,validation,ml_results, "Confusion Matrix - lda")
```


```{r}
ml_results<-evaluate_performance("rpart",train_rpart,validation,ml_results, "Confusion Matrix - rpart")
```

```{r}
ml_results<-evaluate_performance("rf",train_rf,validation,ml_results,"Confusion Matrix - rf")
```

```{r}
ml_results<-evaluate_performance("svmlinear",train_svm,validation,ml_results, "Confusion Matrix - svm")
```
```{r}
ml_results<-evaluate_performance("ada",train_ada,validation,ml_results,"Confusion Matrix - adaboost")
```
```{r}
ml_results<-evaluate_performance("rf_pca",train_rf_pca,validation,ml_results,"Confusion Matrix - rf pca")
```


Now, we do an experiment to combine the predictions of multiple models, i.e., ensemble model. The idea is to diagnose a liver disease only if 50% of the predictions from different models vote that the liver has a disease. The accuracy of the model and the resulting confusion matrix is not good to consider it for further analysis. 

```{r, message=FALSE, warning=FALSE}

# Generating prediction of all models
glm_predictions<-predict(train_glm, validation)
knn_predictions<-predict(train_knn, validation)
loess_predictions<-predict(train_loess, validation)
pls_predictions<-predict(train_pls, validation)
lda_predictions<-predict(train_lda, validation)
rpart_predictions<-predict(train_rpart, validation)
rf_predictions<-predict(train_rf, validation)
svmlinear_predictions<-predict(train_svm, validation)
ada_predictions<-predict(train_ada, validation)
rf_pca_predictions<-predict(train_rf_pca, validation)

# Generate outputs fpr ensemble model
ensemble_pred<-data.frame(glm_predictions, 
                          knn_predictions,
                          loess_predictions,
                          pls_predictions, 
                          lda_predictions,
                          rpart_predictions, 
                          rf_predictions, 
                          svmlinear_predictions,
                          ada_predictions,
                          rf_pca_predictions)

# If 50% of the predictions say disease then we pick it a disease
votes <- rowMeans(ensemble_pred=="M")
ensemble_predictions <- ifelse(votes > 0.5, "M", "B") %>% factor()

# Generate metrics
cm<-confusionMatrix(ensemble_predictions,validation$LiverDisease,positive="M")
draw_confusion_matrix(cm,"Confusion Matrix - ensemble")
sensitivty<-as.numeric(cm$byClass[1])
specificity<-as.numeric(cm$byClass[2])
precision<-as.numeric(cm$byClass[5])
recall<-as.numeric(cm$byClass[6])
f1_score<-as.numeric(cm$byClass[7])
accuracy<-as.numeric(cm$overall[1])
kappa <-as.numeric(cm$overall[2])

# Store metrics to a data frame
ml_results <- bind_rows(ml_results, data_frame(Models = "Ensemble",
             Accuracy = accuracy,
             Precision= precision,
             Sensitivty=sensitivty,
             Specificity=specificity,
             F1_Score = f1_score,
             Kappa= kappa))
```

From the results, we can see that \emph{rf_pca} performs the best compared to the other models. We saw a significant improvement in \emph{kappa} and \emph{Specifivity} measures compared to the other models. Though, the model has a slightly lower sensitivity compared to the other models. 

```{r}
ml_results
```

\section{Conclusion}
\label{sec:conclusion}
In this project, we developed machine learning models to diagnose liver disease by analysing protein levels in the blood. We used patients' liver records collected from India.  We found out that some variables did not correlate with the presence or absence of liver disease. So, we ignored those variables and used the remaining variables to train the model. 

The results show that \emph{rf+pca} performed the best on the validation dataset. We tried to further improve the results by combining the outputs of several models. The idea was to use votes to decide if the liver has a disease. If more than 50% of the models predict a disease, then we consider that the liver is damaged. But, the resulting model did not perform well.

All models seem to overfit the data, and we end up getting more "Malignant" cases. The \emph{rf_pca} performs better, and we can correctly predict some "Benign" cases. The only reason we can think of is data is imbalanced. Ideally, both classes should have an equal distribution for patient records to train robust models. But, only 28% of the data belong to patients with liver disease. That's why the trained models are more biased in wrongly categorizing healthy patients. The ideal solution is to get more data to produce robust models. Alternatively, we can class weights to solve the problem of imbalanced data. We will explore the concept of class weights in future work.

\begin{thebibliography}{9}
\bibitem{cad} 
Ethan Du-Crowa, Lucy Warrenb, Susan M Astleya and Johan Hullemanc,"Is there a safety-net effect with Computer-Aided Detection (CAD)?", Medical Imaging 2019.
\bibitem{ld}
Eugene, R., Sorrell, Michael F.; Maddrey, Willis C., "Schiff's Diseases of the Liver", 10th Edition, Lippincott Williams \& Wilkins by Schiff.

\bibitem{bendi}
Bendi,  Venkata . R, M. S. Prasad Babu, and N. B. Venkateswarlu, "Critical Comparative Study of Liver Patients from USA and INDIA: An Exploratory Analysis", International Journal of Computer Science Issues, May 2012.


\end{thebibliography}


