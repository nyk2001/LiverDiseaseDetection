---
title: "Liver Disease Classification"
author: "Nabeel Khan"
date: "27-May-2020"
output:
  pdf_document: 
    keep_tex: true
    number_sections: yes
    toc: yes
    highlight: tango
    df_print: kable
  html_document:
    df_print: paged
    number_sections: yes
    toc: yes
---
\section{Introduction}
\label{sec:introduction}
This report is part of the ‘HarvardX: PH125.9x Data Science: Capstone’ course. In this report, we chose a dataset of our choice and develop machine learning models to perform binary classification to diagnose liver disease. 

\subsection{Background}
\label{sec:background}
The liver plays an important role in keeping us healthy. The main job of liver is to filter the blood coming from the digestive tract, before passing it to the rest of the body. The liver also turns nutrients into chemicals our body needs, turns food into energy, and filters out poisons. The malfunctioing of liver affects the whole body.

The problems with liver patients are not easily discovered in an early stage. An early diagnosis of liver disease increases the survival rate of patiets. The liver disease can be detected by analyzing the levels of enzymes in the human blood \cite{ld,bendi}. Therefore, a classification algorithm capable of automatically detecting the liver disease can provide assisstance to the doctors in daignosis. The classification techniques are commonly employed in various automatic medical diagnoses tools\cite{cad}. 

\subsection{Aim of Project}
\label{sec:aim}
The patients with liver disease are on the rise because of excessive consumption of
alcohol, inhale of harmful gases, or intake of contaminated food. The aim of this proect is to develop a bianry classifier, which can use blood enzymes information to diagnose liver disease. 

\section{Dataset and Evaluation Metrics}
\label{sec:dataset}
We use the liver patient records, which are collected from North East of Andhra Pradesh, India. The data set contains:
\begin{enumerate}
\item 416 liver patient records and 167 non-liver patient records.
\end{enumerate} 

\subsection{Download Data}
\label{sec:dd}
The dataset is publically available online both at Kaggle and UCI repository.
We download data from the website. Then, we split data into training and validation sets. 
\begin{itemize}
\item 10\% of the data is used for validation, and 90% is used for training.
\end{itemize}

```{r}
################################
#  Install packages (if not installed)
################################
# Note: this process could take a couple of minutes
repos_path<- "http://cran.us.r-project.org"
if(!require(tidyverse)) install.packages("tidyverse", repos =repos_path)
if(!require(caret)) install.packages("caret", repos = repos_path)
if(!require(data.table)) install.packages("data.table", repos =repos_path)
if(!require(lubridate)) install.packages("lubridate", repos = repos_path)
if(!require(dplyr)) install.packages("dplyr", repos = repos_path)
if(!require(sjmisc)) install.packages("dplyr", repos = repos_path)
if(!require(scales)) install.packages("scales", repos = repos_path)
if(!require(caret)) install.packages("caret", repos = repos_path)
if(!require(caretEnsemble)) install.packages("caretEnsemble", repos = repos_path)

################################
# Load libraries
################################
library(lubridate)
library(tidyverse)
library(dplyr)
library(lubridate)
library(sjmisc)
library(scales)
library(caret)
library(caretEnsemble)

```

```{r}
################################
# Downloading data
################################
# Indian Live Patient Records :
 # https://www.kaggle.com/uciml/indian-liver-patient-records/
 # https://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian Liver Patient Dataset (ILPD).csv

url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian Liver Patient Dataset (ILPD).csv"

# Download csv
liverData <- read.csv(url)

# Rename columns of csv
colnames(liverData)<- c("Age","Gender","Total_Bilirubin","Direct_Bilirubin", "Alkaline_Phosphotase","Alamine_Aminotransferase","Aspartate_Aminotransferase",	"Total_Protiens","Albumin","Albumin_and_Globulin_Ratio","Dataset")

################################
# Creating training and validation sets
################################

# Validation set will be 10% of whole data
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(y = liverData$Dataset, times = 1, p = 0.1, list = FALSE)

training <- liverData[-test_index,]
validation <- liverData[test_index,]

 # Removing the objects from environment as no longer required
rm(liverData)

```

\subsection{Metrics}
\label{sec:metrics}
To evaluate the performance of classifiers, we will use following metrics:

\begin{enumerate}
\item \textbf{Accuracy}
It is the ratio of number of correct predictions to the total number of input samples.
\begin{equation}
Accuracy = \frac{True positives + True negatives} {Total Predictions}
\end{equation}

\item \textbf{Sensitivity}
It is also referred as true positive rate or recall. It is the proportion of true positives that are correctly identified.

\begin{equation}
Sensitivity = \frac{Number of true positives} {Number of true positives + Number of false negatives}
\end{equation}

\item \textbf{Precision}
It is defined as the proportion of the true positives against all the positive
results.

\begin{equation}
Precision = \frac{Number of true positives} {Number of true positives + Number of false positives}
\end{equation}

\item \textbf{Specificity}
It is the True negative rate. It is the proportion of true negatives that are
correctly identified.

\begin{equation}
Specificity = \frac{Number of true negatives} {Number of true negatives + Number of false positives}
\end{equation}

\item \textbf{F1 Score}
One metric that is preferred over accuracy is the average of specificity and sensitivity, referred to as balanced accuracy. Because specificity and sensitivity are rates, it is more appropriate to compute the harmonic average. In fact, the F1-score is widely used to compute harmonic average of precision and recall.

\begin{equation}
F1 Score = 2 * \frac{Precision - Recall} {Precision + Recall}
\end{equation}

\end{enumerate}

\section{Data Exploration}
\label{sec:exploration}
The dataset contains 11 variables namely, 'Age','Gender','Total_Bilirubin', or "Alkaline_Phosphotase". The 'Dataset' variable indicates if the liver has a disease or not. For instance, a value of 1 indicates a disease and 2 indicates no disease. 


All other variables except "Age", "Gender", and "Dataset" represent the amount of enzymes or protiens in the blood. These variables (or a subset) will be used to train our machine learning models to make diagnosis. 

\small
```{r}
head(training)
```

The training dataset has 523 records. We can see that "Albumin_and_Globulin_Ratio" variable has 4 null values. The remaining variables have no NULL values. 
\begin{itemize}
\item The validation data has no null values (confirmed via summary).
\end{itemize}

```{r}
sprintf("Rows of training dataset = %d", nrow(training))
print("=========================")
summary(training)
```
```{r}
summary(validation)
```


\subsubsection{Data Wrangling}
\label{sec:dw}
\subsubsubsectionP{Remove null values}
The variable "Albumin_and_Globulin_Ratio" has four null values. We replace null values with the mean of the variable.

```{r}
# Replace null values with the mean
training$Albumin_and_Globulin_Ratio[is.na(training$Albumin_and_Globulin_Ratio)] <- mean(training$Albumin_and_Globulin_Ratio, na.rm=TRUE)

```
\subsubsubsectionP{Create Diagnosis Variable}
The variable "Dataset" indicates if the liver has disease or not. To improve readability, we create a new column namely "LiverDisease", which can have one of the following values: 
\begin{enumerate}
\item Malignant (M) indicating that the patient has a liver disease.
\item Benign (B) indicating that the patient has no no liver disease.
\end{enumerate}


We further delete the "Dataset" variable as it is no longer needed. We apply these operations to both training and validation datasets.

```{r}
# Adding a new column, which will contain the disease information
training <- transform(training, LiverDisease= ifelse(Dataset==1, "M","B"))
validation <- transform(validation, LiverDisease= ifelse(Dataset==1, "M","B"))

# Deleting the column 'Dataset' as no longer required
training<-within(training, rm(Dataset))
validation<-within(validation, rm(Dataset))

# Displaying the first six rows
head(training)
```

\section{Data Analysis}
\label{sec:dataanalysis}
In this section, we extract insights from all variables to get in depth understanding.

\subsection{Age}
The dataset consists of patients with varying ages ranging from 4 to 90. The distribution of ages shows a nice spread and indicates that the dataset is un-biased towards a specific age group. 
```{r}
sprintf("Minimum age = %d",min(training$Age))
sprintf("Maximum age = %d",max(training$Age))
```


```{r}
# Extracting frequency of patient ages
age_stats <-as.data.frame(table(training$Age))
names(age_stats)<- c("Age","Count")

# Remvoing the factor
age_stats$Age<-as.numeric(levels(age_stats$Age))

# Plotting distribution of ages
age_stats %>% ggplot(aes(Age, Count)) +
  geom_point(color="cadetblue") +
  scale_x_continuous(breaks = round(seq(min(age_stats$Age), max(age_stats$Age), by = 6),1)) +
  ggtitle("Distribution of Patient Ages")

```
We breakdown the distrbution of ages with respect to the presence or absence of liver disease. Again, we notice a good spread of age group for both scenarios.   

```{r}
# Plotting distributions of ages versus liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Age, color=LiverDisease)) +
  geom_point() +
  labs(y="Age", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of ages based on liver disease")
```
\subsection{Gender}
76% of the patient records are of males. It would be good to have more and less equal distribution of records for both genders. Although, we do not expect it to make any difference on the performance of our models.  
```{r}
# Getting summary of genders
summary(training$Gender)
```
\subsection{Total_Bilirubin and Direct_Bilirubin}
Bilirubin refears to any form of a yellowish pigment made in the liver when red blood cells are broken down. The elevated levels indicate that liver is damaged.

We can notice that levels of Bilirubin are high for patients with liver diseases.  

```{r}
# Plotting distributions of Total_Bilirubin based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Total_Bilirubin, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Total_Bilirubin", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Total_Bilirubin based on liver disease")

```

```{r}
# Plotting distributions of Direct_Bilirubin based on liver disease
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Direct_Bilirubin, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Direct_Bilirubin", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Direct_Bilirubin based on liver disease")

```
The correlations indicate that both bilirubins are weakly correlated with the liver disease. However, both direct and total bilirubins are highly correlated with each other.
```{r}
# Making a subset of data
subset_train <- training[c("Total_Bilirubin","Direct_Bilirubin","LiverDisease")]
# Converting disease variable to numeric format
subset_train <- transform(subset_train, LiverDisease= ifelse(subset_train$LiverDisease=="M", 1,0))
# Looking at the coorelations
cor(subset_train) 
```
\subsection{Alkaline Phosphotase}
Alkaline phosphatase (ALP) is an enzyme in a person's blood, which helps to break down proteins. We notice that levels of ALP are comparatively high for patients with liver diseases.  

```{r}
# Plotting distributions of Alkaline Phosphotase based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Alkaline_Phosphotase, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Alkaline Phosphotase", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Alkaline Phosphotase w.r.t liver disease")
```
 
\subsection{Alamine_Aminotransferase and Aspartate_Aminotransferase}
Aminotransferases are enzymes that are important in the synthesis of amino acids, which form proteins. Alanine aminotransferase (ALT) and Aspartate aminotransferase (AST) are found primarily in the liver and kidney. A high levels of ALT and AST are expected for patients with liver diseases. We also observe the slightly elevated levels for these enzymes for patients with liver diseases. 

```{r}
# Plotting distributions of Alamine Aminotransferase based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Alamine_Aminotransferase, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Alamine Aminotransferase", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Alamine Aminotransferase based on liver disease") 
```

```{r}
# Plotting distributions of Aspartate_Aminotransferase based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Aspartate_Aminotransferase, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Aspartate Aminotransferase", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Aspartate Aminotransferase based on liver disease") 
```
Contrary to bilirubins, there exists a weak correlation between both aminotransferase. They also have a low correlation with disease compared to bilirubins.

```{r}
# Making a subset of data
subset_train <- training[c("Alkaline_Phosphotase","Aspartate_Aminotransferase","LiverDisease")]
# Converting disease variable to numeric format
subset_train <- transform(subset_train, LiverDisease= ifelse(subset_train$LiverDisease=="M", 1,0))
# Looking at the coorelations
cor(subset_train) 
```

\subsection{Total Protiens}
The total protein test measures the total amount of protien in your body. The distributions indicate that we can disagnose liver disease realibly using this variable. 

```{r}
# Plotting distributions of Total Protiens based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Total_Protiens, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Total Protiens", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Total Protiens based on liver diseases") 
```

\subsection{Albumin}
Albumin is a protein made by the liver to keep fluid in the bloodstream. The low levels of albumin often indicate a problem with liver. We can notice that trend from distributions.

```{r}
# Plotting distributions of Albumin based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Albumin, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Albumin", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Albumin based on liver disease") 
```
\subsection{Albumin_and_Globulin_Ratio}
These proteins are important for body growth, development, and health. They form the structural part of most organs and make up enzymes and hormones that regulate body functions. The low ratios refer to liver issues and we can notice the same from distributions graphs. 

```{r}
# Plotting distributions of Albumin based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Albumin_and_Globulin_Ratio, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Albumin and Globulin Ratio", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Albumin and Globulin Ratio based on liver disease") 
```

\section{Methods}
\label{sec:methods}
Based on discussion in Section \ref{sec:dataanalysis}, we will not use "age" and "Total Protien" variables to train the machine learning models. We remove these variables from both training and validation datasets.

```{r}
# Deleting the column 'Dataset' as no longer required
training<-within(training, rm(Age,Total_Protiens))
validation<-within(validation, rm(Age,Total_Protiens))

```

\subsection{Logistic Regression}
We use logistic regression with a cross validation of 10 folds to train the model. 
```{r}
# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number =10)

# Train logistic regression model
train_glm <- train(LiverDisease ~., 
                   method = "glm",
                   data = training,
                   trControl=control)
sprintf("The accuracy of GLM = %f",train_glm$results$Accuracy)
model_results <- data_frame(method = "glm", Accuracy = train_glm$results$Accuracy)
```
\subsection{KNN}
We use KNN with corss validation of 10 folds to train the model. 

```{r}
# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number = 10)
# Train knn model
train_knn <- train(LiverDisease~ .,
                   method = "knn", 
                   data = training,
                   tuneGrid = data.frame(k = seq(3, 51, 2)),
                   trControl = control)

ggplot(train_knn, highlight = TRUE)
sprintf("The best accuracy = %f",max(train_knn$results$Accuracy))

# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="knn",  
                                     RMSE = max(train_knn$results$Accuracy) ))

```

\subsection{Partial Least Squares (PLS)}
We use PLS with corss validation of 10 folds to train the model. 

```{r}
set.seed(10)
# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number = 10)
# Train the model
train_pls <- train(LiverDisease~.,
                   method = "pls",
                   data = training,
                   preProc = c("center", "scale"),
                   tuneLength = 15,
                   trControl = control)

sprintf("The accuracy of PLS  = %f",max(train_pls$results$Accuracy))

# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="pls",  
                                     RMSE = max(train_pls$results$Accuracy) ))


```
\subsection{Linear Discriminant Analysis (LDA)}
The LDA is a statistical classifier and we use it with a corss validation of 10 folds for training.

```{r}
# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number=10)
# Train the model
train_lda <- train(LiverDisease~., 
                   method = "lda",
                   data = training,
                   trControl = control)

sprintf("The accuracy of lda = %f",max(train_lda$results$Accuracy))

# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="lda",  
                                     RMSE = max(train_lda$results$Accuracy) ))


```

\subsection{Quadratic Discriminant Analysis (QDA)}
The QDA is a statistical classifier and we use it with a corss validation of 5 folds for training.The qda model gives the worse performance on training dataset.

```{r}
# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number=10)
# Train the model
train_qda <- train(LiverDisease~.,
                   method = "qda",
                   data = training,
                   trControl = control)

sprintf("The accuracy of qda = %f",max(train_qda$results$Accuracy))
# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="qda",  
                                     RMSE = max(train_qda$results$Accuracy) ))


```
\subsection{Decision Tress}
We use decision trees with a corss validation of 10 folds for training.

```{r}
# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number = 10)
# Train the model
train_rpart <- train(LiverDisease~., 
                     method = "rpart",
                     data = training,
                     trControl = control,
                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)))
ggplot(train_rpart)
sprintf("The accuracy of decision tree = %f",max(train_rpart$results$Accuracy))

# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="rpart",  
                                     RMSE = max(train_rpart$results$Accuracy) ))


```
\subsection{Random Forests}
We use random forests with a corss validation of 10 folds for training.

```{r}
# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number = 10)
# Train the model
train_rf <- train(LiverDisease~.,
                     method = "rf",
                     data = training,
                     trControl = control,
                     tuneGrid = data.frame(mtry=seq(1,7)),
                     ntree=100)
sprintf("The accuracy of forest tree = %f",max(train_rf$results$Accuracy))
# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="rf",  
                                     RMSE = max(train_rf$results$Accuracy) ))

```
\subsection{Support Vector Machine}
We use support vector machine with a corss validation of 10 folds for training.

```{r}
# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number = 10)
# Train the model
train_svm <- train(LiverDisease~.,
                   data = training,
                   method = "svmLinear",
                   trControl = control)

sprintf("The accuracy of svm = %f",max(train_svm$results$Accuracy))
# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="svm",  
                                     RMSE = max(train_svm$results$Accuracy) ))

```

\subsection{Adaboost}
AdaBoost, short for Adaptive Boosting, is a machine learning meta-algorithm for classification. We train the model with a corss validation of 10 folds.

```{r}
# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number = 10)
# Train the model
train_ada <- train(LiverDisease~.,
                   data = training,
                   method = "ada",
                   trControl = control)

sprintf("The accuracy of adaboost = %f",max(train_ada$results$Accuracy))
# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="ada",  
                                     RMSE = max(train_ada$results$Accuracy) ))

```
The reported accuracies for all models across training dataset are shown in the followin table. The results show that lda model performs the worse. All other models provide an accuracy of around 0.70.The random forest seems to perform the best.

```{r}
model_results
```
\section{Results}
\label{sec:results}


\begin{thebibliography}{9}
\bibitem{cad} 
Ethan Du-Crowa, Lucy Warrenb, Susan M Astleya and Johan Hullemanc,"Is there a safety-net effect with Computer-Aided Detection (CAD)?", Medical Imaging 2019.
\bibitem{ld}
Eugene, R., Sorrell, Michael F.; Maddrey, Willis C., "Schiff's Diseases of the Liver", 10th Edition, Lippincott Williams \& Wilkins by Schiff.

\bibitem{bendi}
Bendi,  Venkata . R, M. S. Prasad Babu, and N. B. Venkateswarlu, "Critical Comparative Study of Liver Patients from USA and INDIA: An Exploratory Analysis", International Journal of Computer Science Issues, May 2012.


\end{thebibliography}


