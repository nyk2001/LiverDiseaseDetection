---
title: "Liver Disease Classification"
author: "Nabeel Khan"
date: "27-May-2020"
output:
  pdf_document: 
    keep_tex: true
    number_sections: yes
    toc: yes
    highlight: tango
    df_print: kable
  html_document:
    df_print: paged
    number_sections: yes
    toc: yes
---
\section{Introduction}
\label{sec:introduction}
This report is part of the ‘HarvardX: PH125.9x Data Science: Capstone’ course. In this report, we chose a dataset of our choice and develop machine learning models to perform binary classification to diagnose liver disease. 

\subsection{Background}
\label{sec:background}
The liver plays a vital role in keeping us healthy. The liver's main job is to filter the blood coming from the digestive tract before passing it to the rest of the body. The liver also turns nutrients into chemicals our body needs, converts food into energy, and filters out poisons. The liver damage affects the whole body.

The problems with liver patients are not easily discovered in an early stage. Early diagnosis of liver disease increases the survival rate of patients. The liver disease can be detected by analyzing the levels of enzymes in the human blood \cite{ld,bendi}. Therefore, a classification algorithm capable of automatically detecting the liver disease can assist the doctors in diagnosis. The classification techniques are commonly employed in various automatic medical diagnoses tools\cite{cad}. 

\subsection{Aim of Project}
\label{sec:aim}
The patients with liver disease are on the rise because of excessive consumption of alcohol, inhale of harmful gases, or intake of contaminated food. This project aims to develop a binary classifier, which can use blood enzymes information to diagnose liver disease. 

\section{Dataset and Evaluation Metrics}
\label{sec:dataset}
We use the liver patient records, which are collected from North East of Andhra Pradesh, India. The data set contains:
\begin{enumerate}
\item 416 liver patient records and 167 non-liver patient records.
\end{enumerate} 

\subsection{Download Data}
\label{sec:dd}
The dataset is publically available online both at Kaggle and UCI repository.
We download data from the website. Then, we split data into training and validation sets. 
\begin{itemize}
\item 10\% of the data is used for validation, and 90% is used for training.
\end{itemize}

```{r,message=FALSE, warning=FALSE}
################################
#  Install packages (if not installed)
################################
# Note: this process could take a couple of minutes
repos_path<- "http://cran.us.r-project.org"
if(!require(tidyverse)) install.packages("tidyverse", repos =repos_path)
if(!require(caret)) install.packages("caret", repos = repos_path)
if(!require(data.table)) install.packages("data.table", repos =repos_path)
if(!require(lubridate)) install.packages("lubridate", repos = repos_path)
if(!require(dplyr)) install.packages("dplyr", repos = repos_path)
if(!require(sjmisc)) install.packages("dplyr", repos = repos_path)
if(!require(scales)) install.packages("scales", repos = repos_path)
if(!require(caret)) install.packages("caret", repos = repos_path)
if(!require(gam)) install.packages("gam", repos = repos_path)

################################
# Load libraries
################################
library(lubridate)
library(tidyverse)
library(dplyr)
library(lubridate)
library(sjmisc)
library(scales)
library(caret)
library(gam)
```

```{r}
################################
# Downloading data
################################
# Indian Live Patient Records :
 # https://www.kaggle.com/uciml/indian-liver-patient-records/
 # https://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian Liver Patient Dataset (ILPD).csv

url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00225/Indian Liver Patient Dataset (ILPD).csv"

# Download csv
liverData <- read.csv(url)

# Rename columns of csv
colnames(liverData)<- c("Age","Gender","Total_Bilirubin","Direct_Bilirubin", "Alkaline_Phosphotase","Alamine_Aminotransferase","Aspartate_Aminotransferase",	"Total_Protiens","Albumin","Albumin_and_Globulin_Ratio","Dataset")

################################
# Creating training and validation sets
################################

# Validation set will be 10% of whole data
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(y = liverData$Dataset, times = 1, p = 0.1, list = FALSE)

training <- liverData[-test_index,]
validation <- liverData[test_index,]

 # Removing the objects from environment as no longer required
rm(liverData)

```

Both training and validation datasets have a good number of records with and with no disease.
```{r}
# Looking at distributions of liver disease
# 1 indicates that the liver is damage. While 2 means that the liver is healthy 
table(training$Dataset)
table(validation$Dataset)

```

\subsection{Metrics}
\label{sec:metrics}
To evaluate the performance of classifiers, we will use the following metrics:

\begin{enumerate}
\item \textbf{Accuracy}
It is the ratio of the number of correct predictions to the total number of input samples.
\begin{equation}
Accuracy = \frac{True positives + True negatives} {Total Predictions}
\end{equation}

\item \textbf{Precision}
It is defined as the proportion of the true positives against all the positive
results.

\begin{equation}
Precision = \frac{Number of true positives} {Number of true positives + Number of false positives}
\end{equation}

\item \textbf{Sensitivity}
It is also referred as true positive rate or recall. It is the proportion of true positives that are correctly identified.

\begin{equation}
Sensitivity = \frac{Number of true positives} {Number of true positives + Number of false negatives}
\end{equation}


\item \textbf{Specificity}
It is the true negative rate. It is the proportion of true negatives that are
correctly identified.

\begin{equation}
Specificity = \frac{Number of true negatives} {Number of true negatives + Number of false positives}
\end{equation}


\begin{equation}
F1 Score = 2 * \frac{Precision - Recall} {Precision + Recall}
\end{equation}

\end{enumerate}

\section{Data Exploration}
\label{sec:exploration}
The dataset contains 11 variables, namely, "Age", "Gender'', "Total_Bilirubin", or "Alkaline_Phosphotase". The 'Dataset' variable indicates if the liver has a disease or not. For instance, a value of 1 means that the liver is damaged, while a value of 2 means that the liver is healthy. 


All other variables except "Age", "Gender", and "Dataset" represent the amount of enzymes or proteins in the blood. These variables (or a subset) will be used to train our machine learning models to make diagnoses. 

```{r}
head(training)
```

The training dataset has 523 records. We can see that the "Albumin_and_Globulin_Ratio" variable has 4 null values. The remaining variables do not contain any null values.

\begin{itemize}
\item The validation data also has no null values (confirmed via summary).
\end{itemize}

```{r}
sprintf("Rows of training dataset = %d", nrow(training))
print("=========================")
summary(training)
```

```{r}
summary(validation)
```


\subsection{Data Wrangling}
\label{sec:dw}
\subsubsection{Remove null values}
The variable "Albumin_and_Globulin_Ratio" has four null values. We use the traditional data science approach and replace null values with the mean of the variable.

```{r}
# Replace null values with the mean
training$Albumin_and_Globulin_Ratio[is.na(training$Albumin_and_Globulin_Ratio)] <- mean(training$Albumin_and_Globulin_Ratio, na.rm=TRUE)
```

\subsubsection{Create Diagnosis Variable}
To improve readability, we create a new column, namely, "LiverDisease", which can have one of the following values: 
\begin{enumerate}
\item Malignant (M) indicating that the patient has liver disease.
\item Benign (B) indicating that the patient has no liver disease.
\end{enumerate}


We further delete the "Dataset" variable as it is no longer needed. We apply these operations to both training and validation datasets.

```{r}
# Adding a new column, which will contain the disease information
# M -> Malignant
# B -> Benign
training <- transform(training, LiverDisease= ifelse(Dataset==1, "M","B"))
validation <- transform(validation, LiverDisease= ifelse(Dataset==1, "M","B"))

# Deleting the column 'Dataset' as no longer required
training<-within(training, rm(Dataset))
validation<-within(validation, rm(Dataset))

# Displaying the first six rows
head(training)
```

\section{Data Analysis}
\label{sec:dataanalysis}
In this section, we extract insights from all variables to get an in-depth understanding before using them to train the machine learning models.

\subsection{Age}
The dataset consists of patients with varying ages ranging from 4 to 90. The distribution of ages shows a nice spread and indicates that the dataset is unbiased towards a specific age group. 
```{r}
sprintf("Minimum age = %d",min(training$Age))
sprintf("Maximum age = %d",max(training$Age))
```


```{r}
# Extracting frequency of patient ages
age_stats <-as.data.frame(table(training$Age))
names(age_stats)<- c("Age","Count")

# Remvoing the factor
age_stats$Age<-as.numeric(levels(age_stats$Age))

# Plotting distribution of ages
age_stats %>% ggplot(aes(Age, Count)) +
  geom_point(color="cadetblue") +
  scale_x_continuous(breaks = round(seq(min(age_stats$Age), 
                                        max(age_stats$Age), by = 6),1)) +
  ggtitle("Distribution of Patient Ages")

```
We breakdown the distribution of ages to the presence or absence of liver disease. We notice a good spread of age group for both scenarios.

```{r}
# Plotting distributions of ages based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Age, color=LiverDisease)) +
  geom_point() +
  labs(y="Age", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of ages based on liver disease")
```

\subsection{Gender}
76% of the patient records are of males. It would be good to have a more and less equal distribution of records for both genders, although we do not expect it to make any difference in our models.  

Both "Gender" and "Age" variables are not used to train the model. These variables provide descriptive information.

```{r}
# Getting summary of genders
summary(training$Gender)
```

\subsection{Total_Bilirubin and Direct_Bilirubin}
Bilirubin refers to any form of a yellowish pigment made in the liver when red blood cells are broken down. The elevated levels of bilirubin indicate that the liver is damaged. We find a similar trend with the variable that levels of bilirubin are high for patients with liver diseases.  

```{r}
# Plotting distributions of Total_Bilirubin based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Total_Bilirubin, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Total_Bilirubin", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Total_Bilirubin based on liver disease")

```

```{r}
# Plotting distributions of Direct_Bilirubin based on liver disease
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Direct_Bilirubin, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Direct_Bilirubin", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Direct_Bilirubin based on liver disease")

```

Now, we look at the correlations of bilirubin. We observe that both bilirubins are weakly correlated with liver diagnosis. However, both bilirubins are highly correlated, and we can also use one of them to train the model (if required).

```{r}
# Making a subset of data
subset_train <- training[c("Total_Bilirubin","Direct_Bilirubin","LiverDisease")]
# Converting disease variable to numeric format
subset_train <- transform(subset_train, LiverDisease= ifelse(subset_train$LiverDisease=="M", 1,0))
# Looking at the coorelations
cor(subset_train) 
```
\subsection{Alkaline Phosphotase}
Alkaline phosphatase (ALP) is an enzyme in a person's blood that helps break down proteins. The elevated levels indicate that the liver has a disease, and we notice a similar trend in our dataset.  

```{r}
# Plotting distributions of Alkaline Phosphotase based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Alkaline_Phosphotase, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Alkaline Phosphotase", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Alkaline Phosphotase based on liver disease")
```
 
\subsection{Alamine_Aminotransferase and Aspartate_Aminotransferase}
Aminotransferases are enzymes that are important in the synthesis of amino acids, which form proteins. Alanine aminotransferase (ALT) and Aspartate aminotransferase (AST) are found primarily in the liver and kidney. High levels of ALT and AST are expected for patients with liver diseases. We also notice slightly elevated levels of these enzymes for patients with liver diseases in our dataset. 

```{r}
# Plotting distributions of Alamine Aminotransferase based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Alamine_Aminotransferase, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Alamine Aminotransferase", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Alamine Aminotransferase based on liver disease") 
```

```{r}
# Plotting distributions of Aspartate_Aminotransferase based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Aspartate_Aminotransferase, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Aspartate Aminotransferase", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Aspartate Aminotransferase based on liver disease") 
```

Contrary to bilirubins, there exists a weak correlation between both aminotransferases. 

```{r}
# Making a subset of data
subset_train <- training[c("Alkaline_Phosphotase","Aspartate_Aminotransferase","LiverDisease")]

# Converting disease variable to numeric format
subset_train <- transform(subset_train, LiverDisease= ifelse(subset_train$LiverDisease=="M", 1,0))

# Looking at the coorelations
cor(subset_train) 
```

\subsection{Total Protiens}
The total protein test measures the total amount of protein in your body. The distributions indicate that this variable cannot be used to diagnose liver disease. We do not see any pattern which we can use for classification.  


```{r}
# Plotting distributions of Total Protiens based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Total_Protiens, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Total Protiens", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Total Protiens based on liver diseases") 
```

\subsection{Albumin}
Albumin is a protein made by the liver to keep fluid in the bloodstream. The low levels of albumin indicate a problem with the liver, and we notice a similar trend in our dataset.

```{r}
# Plotting distributions of Albumin based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Albumin, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Albumin", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Albumin based on liver disease") 
```

\subsection{Albumin_and_Globulin_Ratio (AG)}
These proteins are crucial for body growth, development, and health. They form the structural part of most organs and makeup enzymes that regulate body functions. The low ratios of AG refer to liver issues, and we can notice the same from distributions plot. 

```{r}
# Plotting distributions of Albumin based on liver diseases
training %>% 
  ggplot(aes(as.numeric(row.names(training)),Albumin_and_Globulin_Ratio, color=LiverDisease)) +
  geom_boxplot() +
  labs(y="Albumin and Globulin Ratio", x = "Number of patients")+
  facet_wrap( ~ LiverDisease) +
  ggtitle("Distribution of Albumin and Globulin Ratio based on liver disease") 
```

\section{Methods}
\label{sec:methods}
Based on the discussion in Section \ref{sec:dataanalysis}, we will not use "Age" and "Total Protein" variables to train the machine learning models. We remove these variables from both training and validation datasets.

```{r}
# Deleting the variables 'Age' and 'Dataset' 
training<-within(training, rm(Age,Total_Protiens))
validation<-within(validation, rm(Age,Total_Protiens))

```


For data pre-processing, we remove zero-variance predictors and then center and scale all those remaining in this exact order using the preProc argument. Feature scaling is one of the most critical steps during the pre-processing of data before creating a machine learning model. It can make a significant difference between a weak machine learning model and a better one.

\subsection{Logistic Regression}
We use \emph{glm} method with cross-validation of 10 folds to train the model.

```{r,warning=FALSE}
# Defining a cross-validation (10 K folds )
control <- trainControl(method = "repeatedcv", number =10,repeats = 5)

# Train logistic regression model
train_glm <- train(LiverDisease ~., 
                   method = "glm",
                   preProc = c("zv","center", "scale"),
                   data = training,
                   trControl=control)
sprintf("The accuracy of GLM = %f",train_glm$results$Accuracy)
model_results <- data_frame(method = "glm", Accuracy = train_glm$results$Accuracy)
```
\subsection{K-nearest neigbors (knn)}
We use \emph{knn} method with cross-validation of 10 folds to train the model. We experiment with several values of \emph{k}, ranging from 3 to 51 to get the best model.


```{r,warning=FALSE}

set.seed(1)
# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number = 10, repeats=5)

# Train knn model
train_knn <- train(LiverDisease~ .,
                   method = "knn", 
                   preProc = c("zv","center", "scale"),
                   data = training,
                   tuneGrid = data.frame(k = seq(3, 51, 2)),
                   trControl = control)

# Plot the model and highlight the best result
ggplot(train_knn, highlight = TRUE) +
  ggtitle(paste("The best accuracy = ",round(max(train_knn$results$Accuracy),3)))

# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="knn",  
                                     Accuracy = max(train_knn$results$Accuracy) ))

```
\subsection{Local Regression (loess)}
We use \emph{loess} methof with cross-validation of 10 folds to train the model. We also optimize the span parameter of the method.

```{r, warning=FALSE}
set.seed(1)
# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number = 10, repeats=5)

# Define tuning parameters
tune_grid <- expand.grid(span = seq(0.15, 0.65, len = 15), degree = 1)

# Train the model
train_loess <- train(LiverDisease~.,
                   method = "gamLoess",
                   preProc = c("zv","center", "scale"),
                   data = training,
                   tuneGrid= tune_grid, 
                   trControl = control)

# Plot the model and highlight the best result
ggplot(train_loess, highlight = TRUE) +
  ggtitle(paste("The best accuracy = ",round(max(train_loess$results$Accuracy),3)))


# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="loess",  
                                     Accuracy = max(train_loess$results$Accuracy) ))


```

\subsection{Partial Least Squares (PLS)}
We use \emph{pls} method with cross-validation of 10 folds to train the model. 

```{r}
set.seed(1)

# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number = 10, repeats=5)

# Train the model
train_pls <- train(LiverDisease~.,
                   method = "pls",
                   preProc = c("zv","center", "scale"),
                   tuneLength = 15,
                   trControl = control)

# Plot the model and highlight the best result
ggplot(train_loess, highlight = TRUE) +
  ggtitle(paste("The best accuracy = ",round(max(train_pls$results$Accuracy),3)))


# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="pls",  
                                     Accuracy = max(train_pls$results$Accuracy) ))


```

\subsection{Linear Discriminant Analysis (LDA)}
The \emph{lda} is a statistical classifier, and we use this method with a cross-validation of 10 folds for training.

```{r}
# Defining a cross validation (10 K folds )
control <- trainControl(method = "repeatedcv", number=10, repeats=5)

# Train the model
train_lda <- train(LiverDisease~., 
                   method = "lda",
                   preProc = c("zv","center", "scale"),
                   data = training,
                   trControl = control)

sprintf("The accuracy of lda = %f",max(train_lda$results$Accuracy))

# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="lda",  
                                     Accuracy = max(train_lda$results$Accuracy) ))


```

\subsection{Quadratic Discriminant Analysis (QDA)}
The \emph{qda} is a statistical classifier, and we use the method with a cross-validation of 10 folds for training.

```{r}

# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number=10)

# Train the model
train_qda <- train(LiverDisease~.,
                   method = "qda",
                   preProc = c("zv","center", "scale"),
                   data = training,
                   trControl = control)

sprintf("The accuracy of qda = %f",max(train_qda$results$Accuracy))

# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="qda",  
                                     Accuracy = max(train_qda$results$Accuracy) ))


```
\subsection{Decision Tress}

We use \emph{raprt} method with a cross-validation of 10 folds for training. We also attempt to optimise the \emph{cp} parameter of the model.

```{r}
# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number = 10)
# Train the model
train_rpart <- train(LiverDisease~., 
                     method = "rpart",
                     preProc = c("zv","center", "scale"),
                     data = training,
                     preProc = c("zv","center", "scale"),
                     trControl = control,
                     tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)))

# Plot the model and highlight the best result
ggplot(train_rpart, highlight = TRUE) +
  ggtitle(paste("The best accuracy = ",round(max(train_rpart$results$Accuracy),3)))

# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="rpart",  
                                     Accuracy = max(train_rpart$results$Accuracy) ))


```
\subsection{Random Forests}
We use \emph{rf} method with a cross-validation of 10 folds for training. We also tune the \emph{cp} parameter of the model.

```{r}
# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number = 10)
# Train the model
train_rf <- train(LiverDisease~.,
                     method = "rf",
                     preProc = c("zv","center", "scale"),
                     data = training,
                     trControl = control,
                     tuneGrid = data.frame(mtry=seq(1,7)),
                     ntree=100)

# Plot the model and highlight the best result
ggplot(train_rf, highlight = TRUE) +
  ggtitle(paste("The best accuracy = ",round(max(train_rf$results$Accuracy),3)))

# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="rf",  
                                     Accuracy = max(train_rf$results$Accuracy) ))

```
\subsection{Support Vector Machine}
We use support vector machine with a cross-validation of 10 folds for training.

```{r}
# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number = 10)

# Train the model
train_svm <- train(LiverDisease~.,
                   preProc = c("zv","center", "scale"),
                   data = training,
                   method = "svmLinear",
                   trControl = control)

sprintf("The accuracy of svm = %f",max(train_svm$results$Accuracy))

# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="svm",  
                                     Accuracy = max(train_svm$results$Accuracy) ))

```

\subsection{Adaptive Boosting (Adaboost)}
AdaBoost is a machine learning meta-algorithm for classification. We train the model with a cross-validation of 10 folds.

```{r}
# Defining a cross validation (10 K folds )
control <- trainControl(method = "cv", number = 10)
# Train the model
train_ada <- train(LiverDisease~.,
                   preProc = c("zv","center", "scale"),
                   data = training,
                   method = "ada",
                   trControl = control)

sprintf("The accuracy of adaboost = %f",max(train_ada$results$Accuracy))

# Stroing the results
model_results <- bind_rows(model_results,data_frame(method="ada",  
                                     Accuracy = max(train_ada$results$Accuracy) ))

```
The reported accuracies for all models across training dataset are shown in the following table. The results show that lda model performs the worse. All other models provide an accuracy of around 0.70.The random forest seems to perform the best.

```{r}
model_results
```
\section{Results}
\label{sec:results}
The statistical measurements of accuracy and precision reveal the basic reliability of a test. The specificity is the ability of a test to correctly exclude individuals who do not have a given disease. While sensitivity is the ability of a test to correctly identify people who have a given disease.  

In the Section \ref{sec:methods}, we trained a number of models using training data. Now, we will evaluate the performance of these models using validation data. We will not use \textbf{lda} model due to poor performance. The results show that \textbf{rf} model performs best in terms of precision and recall. However, it has a slitghly lower sensitivity and specificity compared to other models.

```{r}
# Creating an empty data frame to hold the results of models 
# across validation dataset
ml_results<-data_frame()

# Function to compute all stats from models
evaluate_performance <- function(model_name, model, validation,model_results) 
{
  # Generating predictions
  predictions<-predict(model, validation)
  
  # Generate metrics
  accuracy<-confusionMatrix(predictions,validation$LiverDisease,positive="M")$overall["Accuracy"]
  precision <- posPredValue(predictions, validation$LiverDisease,positive="M")
  sensitivty <- sensitivity(predictions, validation$LiverDisease,positive="M")
  specificity<- specificity(predictions, validation$LiverDisease,positive="M")

  # Store metrics to a data frame
  ml_results <- bind_rows(ml_results, data_frame(Models = model_name, 
             Accuracy = accuracy,
             Precision= precision,
             Sensitivty=sensitivty,
             Specificity=specificity))
}

# Evaluating the performance of models
ml_results<-evaluate_performance("glm",train_glm,validation,ml_results)
ml_results<-evaluate_performance("knn",train_knn,validation,ml_results)
ml_results<-evaluate_performance("pls",train_pls,validation,ml_results)
ml_results<-evaluate_performance("lda",train_lda,validation,ml_results)
ml_results<-evaluate_performance("rpart",train_rpart,validation,ml_results)
ml_results<-evaluate_performance("rf",train_rf,validation,ml_results)
ml_results<-evaluate_performance("svmlinear",train_svm,validation,ml_results)
ml_results<-evaluate_performance("ada",train_ada,validation,ml_results)
ml_results
```
Now, we try to combine the predictions of mulitple models i.e. ensemble model. The idea is to diagnose a disease only if 50% of the predictions from different models diagnose a liver disease otherwise not. We can see that the performance of ensemble model is not very good.
```{r}
# Generating prediction of all models
glm_predictions<-predict(train_glm, validation)
knn_predictions<-predict(train_knn, validation)
pls_predictions<-predict(train_pls, validation)
lda_predictions<-predict(train_lda, validation)
rpart_predictions<-predict(train_rpart, validation)
rf_predictions<-predict(train_rf, validation)
svmlinear_predictions<-predict(train_svm, validation)
ada_predictions<-predict(train_ada, validation)

# Generate outputs fpr ensemble model
ensemble_pred<-data.frame(glm_predictions, knn_predictions,
                          pls_predictions, lda_predictions,
                          rpart_predictions, rf_predictions, 
                          svmlinear_predictions,ada_predictions)

# If 50% of the predictions say disease then we pick it a disease
votes <- rowMeans(ensemble_pred=="M")
ensemble_predictions <- ifelse(votes > 0.5, "M", "B") %>% factor()

# Generate metrics
accuracy<-confusionMatrix(ensemble_predictions,validation$LiverDisease,positive="M")$overall["Accuracy"]
precision <- posPredValue(ensemble_predictions, validation$LiverDisease,positive="M")
sensitivty <- sensitivity(ensemble_predictions, validation$LiverDisease,positive="M")
specificity<- specificity(ensemble_predictions, validation$LiverDisease,positive="M")

# Store metrics to a data frame
ml_results <- bind_rows(ml_results, data_frame(Models = "Ensemble", 
             Accuracy = accuracy,
             Precision= precision,
             Sensitivty=sensitivty,
             Specificity=specificity))
ml_results
  
```
\section{Conclusion}
\label{sec:conclusion}
In this project, we developed machine learning models to diagnose liver disease by analysing protien levels in the blood. We used patients liver record collected from India.  We found out that some variables had no correlation with the presence of absence of liver disease. So, we ignored those variables and used the remianing variables to trian the model. 

We achieved an accuracy of around 72% on training dataset. The best accuracy of 69% was reported with \emph{rf} model on validation dataset. Some models, such as pls, rpart performed really well in terms of sensitivity and specificity. We tried tp further improve the results by combining the outputs of several models. But we saw a drop in the performance. 

Unfortunately, we could not improve the accuracy of our models.The box plots have showed that we cannot reliably seperate liver diseases records with variables. So, we will need more data to improve the performance of our models. 



\begin{thebibliography}{9}
\bibitem{cad} 
Ethan Du-Crowa, Lucy Warrenb, Susan M Astleya and Johan Hullemanc,"Is there a safety-net effect with Computer-Aided Detection (CAD)?", Medical Imaging 2019.
\bibitem{ld}
Eugene, R., Sorrell, Michael F.; Maddrey, Willis C., "Schiff's Diseases of the Liver", 10th Edition, Lippincott Williams \& Wilkins by Schiff.

\bibitem{bendi}
Bendi,  Venkata . R, M. S. Prasad Babu, and N. B. Venkateswarlu, "Critical Comparative Study of Liver Patients from USA and INDIA: An Exploratory Analysis", International Journal of Computer Science Issues, May 2012.


\end{thebibliography}


